# 性能测试和配置

## 关于

讨论使用FreeSWITCH™进行性能测试的内容，附带测试场景的开源项目链接。

点击这里展开目录

* 1 [性能指标](#-freeswitch--%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%92%8C%E9%85%8D%E7%BD%AE-)  
   * 1.1 [每秒呼叫数 (CPS)](#-freeswitch--%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%92%8C%E9%85%8D%E7%BD%AE-)  
   * 1.2 [并发呼叫](#-freeswitch--%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%92%8C%E9%85%8D%E7%BD%AE-)  
   * 1.3 [多线程](#-freeswitch--%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%92%8C%E9%85%8D%E7%BD%AE-)
* 2 [配置](#-freeswitch--%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%92%8C%E9%85%8D%E7%BD%AE-)  
   * 2.1 [推荐配置](#推荐配置)  
   * 2.2 [推荐ULIMIT设置](#推荐ulimit设置)  
   * 2.3 [Linux中以太网调优](#-freeswitch--%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%92%8C%E9%85%8D%E7%BD%AE-)  
   * 2.4 [TCP/IP调优](#-freeswitch--%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%92%8C%E9%85%8D%E7%BD%AE-)  
   * 2.5 [FreeSWITCH的core.db I/O瓶颈](#freeswitchs-coredb-io-%E7%93%B6%E9%A2%88)
* 3 [压力测试](#-freeswitch--%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%92%8C%E9%85%8D%E7%BD%AE-)
* 4 [参考链接](#-freeswitch--%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%92%8C%E9%85%8D%E7%BD%AE-)

## 性能指标

当人们谈论性能时，它可能涵盖了很多不同的方面。实际上，性能通常取决于两个瓶颈，即SIP和RTP。这些通常分别转化为每秒呼叫数和并发呼叫。此外，高负载系统可能会出现与数据库服务器的连接耗尽或查找帐户或配置数据时的带宽瓶颈。

### 每秒呼叫数 (CPS)

由于每秒呼叫数量仅仅是衡量每秒建立和关闭呼叫的指标，所以限制因素是处理SIP消息的能力。根据您所拥有的流量类型，这可能是个因素，也可能不是。有各种组件可以导致这个瓶颈，其中只有一部分是FreeSWITCH及其库。

### 并发呼叫

使用现代硬件，同时进行的呼叫数量不是SIP的限制，而是RTP媒体流的限制。这可以细分为可用带宽和每秒数据包数。通过千兆以太网端口，[理论上](http://www.bandcalc.com/)的并发呼叫限制大约为10,500个呼叫（不包含RTCP），假设使用G.711编解码器和链路层开销。理论很美好，但在现实中，内核网络层将成为限制因素，原因是RTP媒体流的每秒数据包数。

### 多线程

FreeSWITCH使用线程。在现代Linux内核中，线程和进程分叉很相似。通常，“top”实用程序只显示一个FS进程，因为默认情况下，“top”将所有线程合并到一个条目中。命令“top -H”将显示各个线程。

默认情况下，“htop”会显示各个线程。

在FreeSWITCH中，有几个线程只在基本空闲的FreeSWITCH进程上运行。每个额外的呼叫通路至少需要一个线程。根据呼叫中激活的应用程序，每个呼叫通路可能有多个线程。

## 配置

### 推荐配置

建议使用64位的CPU运行64位操作系统，并安装64位版本的FreeSWITCH。裸金属系统能提供一致可靠的性能，尤其对于实时应用程序而言，能提供可靠的内核时钟用于RTP数据包定时。虚拟机很难确定问题的源头，并且可能无法正确传递硬件时钟给客户操作系统，因此RTP测试的结果可能毫无意义。

推荐使用Debian Linux作为操作系统，因为核心开发者使用它并进行了最全面的测试。当然，FreeSWITCH也可以在其他操作系统上运行。

### 推荐的ULIMIT设置

以下是FreeSWITCH的推荐ULIMIT设置，可用于最大化性能。您可以在do\_start()前的initd脚本中添加ulimit设置。

```bash
ulimit -c unlimited # 创建核心文件的最大大小。
ulimit -d unlimited # 进程数据段的最大大小。
ulimit -f unlimited # Shell创建文件的最大大小（默认选项）。
ulimit -i unlimited # 挂起信号的最大数量。
ulimit -n 999999    # 打开文件描述符的最大数量。
ulimit -q unlimited # POSIX消息队列的最大大小。
ulimit -u unlimited # 单个用户可用的进程最大数量。
ulimit -v unlimited # 进程可用的虚拟内存的最大量。
ulimit -x unlimited # ???
ulimit -s 240         # 栈的最大大小。
ulimit -l unlimited # 可锁定进程内存的最大大小。
ulimit -a           # 报告所有当前限制。
```

### Linux中的以太网调优

注意缓冲区膨胀（buffer bloat）。

在`bufferbloat`的专家来找我们之前，这里曾经有一条提示，建议将缓冲区设置为最大。这个建议在很多方面都是错误的。长话短说，在实时媒体（如VoIP）中，绝对不希望使用大缓冲区。在未饱和的网络链路上，您可能不会注意到任何问题，但当您的网络链路饱和时，较大的缓冲区将导致RTP数据包被缓冲而不是丢弃。

那么，您的rx/tx queuelens应该是多少呢？只有您自己能够确定，但进行一些实验是一个好主意。通常在Linux中，它默认为1000。如果您使用良好的流量整形队列(qdisc)（如`pfifo_fast`或`SFB`等）并对UDP/RTP数据流进行优先处理，您可以将其保持不变，但对于VoIP应用程序来说，根据您的工作负载和互联性，降低它可能是一个好主意。

无论如何，请不要使用默认的FIFO队列（pfifo qdisc）。它按严格的FIFO顺序输出数据包。

要查看当前的设置，请使用ethtool命令：

**ethtool设置**

```bash
[root@server:~]# ethtool -g eth0
eth0的环参数：
预设最大值：
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             256
当前硬件设置：
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             128
```

这是我在我的Lenny安装中使用的默认设置。如果您需要更改它，可以执行以下操作：

**ethtool建议的更改**

```bash
[root@server:~]# ethtool -G eth0 rx 128
[root@server:~]# ethtool -g eth0
eth0的环参数：
预设最大值：
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             256
当前硬件设置：
RX:             128
RX Mini:        0
RX Jumbo:       0
TX:             128
```

[http://tcpipguide.com](http://tcpipguide.com)  is an excellent resource for understanding TCP/IP tuning. One aspect that can have a significant impact on performance is the size of the ring buffers. This refers to the buffers used by the network interface card (NIC) to store incoming and outgoing packets.

There is no one-size-fits-all answer to what the ideal ring buffer size should be. It depends on the specific network traffic and the characteristics of the system. However, based on observations and experiences, it is generally recommended to limit the number of unmanaged TX (transmit) buffers to no more than 32 on a 100Mbit network. This recommendation is supported by research papers such as [http://www.cs.clemson.edu/\~jmarty/papers/bittorrentBroadnets.pdf](http://www.cs.clemson.edu/%7Ejmarty/papers/bittorrentBroadnets.pdf) and insights from Dave Taht from the Bufferbloat project.

For example, on the Lenny system mentioned, the current TX buffer settings are 32/64. Unfortunately, it is not possible to reduce the TX buffer further with this particular driver. Consequently, there may be a latency of approximately 10ms in the driver itself for packets on a 100Mbit network.

It's worth noting that a large TXQUEUELEN (TX queue length) can also introduce significant delays. Therefore, it is advisable to keep the TX buffer size at an optimal level.

On a gigabit network interface, the default TX queues and TXQUEUELEN are generally more appropriate but may still be excessively large.

In terms of RX (receive) buffers, having larger buffers can be beneficial to handle bursts of incoming packets without packet loss. However, it is essential to monitor and fine-tune the RX buffer size based on actual packet loss on the receive channel.

Lastly, it is important to consider that the optimal TX buffer size varies depending on the link speed. For example, on a 3Mbit uplink, the ideal buffer size is much lower than on a 100Mbit uplink. In fact, the debloat-testing kernel offers some Ethernet and wireless drivers that allow reducing the TX buffer size to as low as 1. This can help improve performance in specific scenarios.

In conclusion, TCP/IP tuning involves finding the right balance for ring buffer sizes based on the specific network conditions and requirements. The information and recommendations provided here serve as a starting point to optimize network performance. For more in-depth discussions and guidance on TCP/IP tuning, [http://tcpipguide.com](http://tcpipguide.com) is a valuable resource.

对于主要用于VoIP的服务器来说，TCP Cubic（Linux的默认选项）可能会对网络子系统造成过大的压力。使用基于测得延迟进行递增的TCP Vegas被一些FreeSWITCH用户在生产中使用，作为“更友好、更温和”的用于命令和控制功能的TCP。

要启用Vegas而不是Cubic，您可以在启动时执行以下操作：

modprobe tcp_vegas

echo vegas > /proc/sys/net/ipv4/tcp_congestion_control

--- 关于tcp_vegas的一些有趣评论，请参考 <http://tomatousb.org/forum/t-267882/>

### FreeSWITCH的core.db I/O瓶颈

在正常配置下，core.db几乎每秒都会写入磁盘，每秒生成数百个块写入。为了避免这个问题，请将/usr/local/freeswitch/db转换为内存文件系统。如果您使用的是SSD，将core.db移到RAM磁盘以延长SSD的使用寿命至关重要。

在当前的FreeSWITCH版本中，您应该使用switch.conf.xml中记录的"core-db-name"参数（只需重新启动FreeSwitch以应用更改）：

   &lt;param name="core-db-name" value="/dev/shm/core.db" />

否则，您可以创建一个专用的内存文件系统，例如将以下内容添加到/etc/fstab的末尾

**fstab示例**

```bash
#
# /etc/fstab条目示例（使用默认大小）
#
tmpfs /usr/local/freeswitch/db tmpfs defaults 0 0
#
# 若要为文件系统指定大小，请使用适当的mount(1)选项：
#
# tmpfs /usr/local/freeswitch/db tmpfs defaults,size=4g 0 0
#
```

要使用新的文件系统，请运行以下命令（或根据您的操作系统运行等效的命令）：

mount /usr/local/freeswitch/db
/etc/init.d/freeswitch restart

另一个方案是将核心数据库移入ODBC数据库，这将使这个处理过程转移到一个能更好地处理大量请求的DBMS，并且甚至可以将这个处理过程转移到另一台服务器上。考虑使用[freeswitch.dbh](../../Databases/Lua-FreeSWITCH-Dbh_3965358.mdx#about)来充分利用连接池。

## 压力测试

知道如何正确进行压力测试非常重要，否则你的结果将毫无意义。

使用SIPp是黑魔法、巫术和Santeria的结合。你已经被警告了。

在使用SIPp的uas和uac测试FreeSWITCH时，需要确保有来回的媒体流动。如果你只是从一个sipp发送媒体到另一个sipp而没有回送RTP(-rtp\_echo)，FS会由于媒体超时而超时。这是为了防止当一方没有媒体超过一定时间时出现错误计费。

## 参见

[在PC Engines APU上测试FreeSWITCH性能](https://txlab.wordpress.com/2014/04/18/freeswitch-performance-test-on-pc-engines-apu/) — Stanislav Sinyagin使用一台测试机器测试了FreeSWITCH™的转码性能。

[现实世界的观察结果](./Real-world-results_13173614.mdx#-freeswitch--real-world-results-) — 发布在使用FreeSWITCH™进行真实世界配置而不是压力测试时的测量结果。

[Linux的SSD调优](../SSD-Tuning-for-Linux_1966304.mdx#about) — 用于使用固态硬盘进行存储的系统的特殊考虑事项。

[SIPp](http://sipp.sourceforge.net/) — 用于SIP的开源测试工具和流量生成器。

[SIPpy Cup](https://github.com/mojolingo/sippy%5Fcup) — Ben Langfeld贡献了这个用于SIPp的场景生成器，以简化测试配置文件的创建，特别是兼容的媒体。

[check\_voip\_call](https://github.com/bbhenry/check%5Fvoip%5Fcall) — Henry Huang为Nagios贡献了这个项目

<http://www.bandcalc.com/> — 用于不同编解码器和使用情况的带宽计算器

<http://www.cs.clemson.edu/%7Ejmarty/papers/bittorrentBroadnets.pdf> — 基于BitTorrent使用的缓冲区大小论文